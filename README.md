# Language Model

This python notebook showcases an implementation of a simple language model using a trigram approach. The project includes a [Google Colab notebook](Language_Model.ipynb) where you can explore the code and experiment interactively.

## Overview

- **Trigram Language Model:**  
  This model built using trigrams, which predicts the next word in a sequence based on the previous two words. This classic NLP technique helps in understanding the basics of how statistical language models work.

- **Learning LLM & Transformers:**  
  As part of this, I also learned about modern Large Language Models (LLMs) and Transformer architectures. However, the implemented model here is a foundational n-gram approach, which serves as a stepping stone for me towards understanding more complex models like Transformers.

## Whatâ€™s Included

- [Language_Model.ipynb](Language_Model.ipynb):  
  A Jupyter/Colab notebook demonstrating the trigram model with Alice in Wonderland plain text dataset (from [Project Gutenberg](https://www.gutenberg.org/) .

## Getting Started

1. **Open the Notebook:**  
   You can run and modify the notebook directly in [Google Colab](https://colab.research.google.com/) for an interactive experience.

2. **Requirements:**  
   - Python 3.x  
   - Jupyter Notebook or Google Colab account

3. **How to Use:**  
   - Follow the notebook cells to understand each step of the trigram model implementation.
   - Experiment with different text data of different sizes like:
      - [Penn Treebank](https://catalog.ldc.upenn.edu/LDC99T42)
      - [Wikipedia Corpus](https://www.kaggle.com/datasets/emmermarcell/wikipedia-corpus-2023-03-01)
   - and see how model performs.
## Learning Resources

[LLMs](https://www.kaggle.com/whitepaper-foundational-llm-and-text-generation)
[Attention All you need paper](https://arxiv.org/abs/1706.03762)


## License

This project is for learning purposes only.
